{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sklearn.feature_extraction.text as text;\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from sklearn import linear_model\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import threading  \n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "    \n",
    "class Loader:\n",
    "    def __init__(self,):\n",
    "        train_file = 'train.csv';\n",
    "        test_file = 'test.csv';\n",
    "        self.item2id = {}\n",
    "        self.id2item = {}\n",
    "        self.item_title = {}\n",
    "        self.item_text = {}\n",
    "        self.test_id = []\n",
    "        self.train = self.load(train_file);\n",
    "        self.test = self.load(test_file);\n",
    "        self.load_description();\n",
    "        self.title = []\n",
    "        \n",
    "        print('begin proprocessing')\n",
    "        self.preprocess();\n",
    "        print('compute cosine')\n",
    "        self.train_cosine = self.compute_cosine([self.train_s, self.item_title_tfidf, self.item_text_tfidf], self.train);\n",
    "        self.train_cosine = np.asarray(self.train_cosine);\n",
    "        self.test_cosine = self.compute_cosine([self.test_s, self.item_title_tfidf, self.item_text_tfidf], self.test);\n",
    "        self.test_cosine = np.asarray(self.test_cosine)\n",
    "        print('finish')\n",
    "        \n",
    "        \n",
    "    def load(self, file_name):\n",
    "        reader = csv.reader(open(file_name, encoding='latin-1'));\n",
    "        cnt = 0;\n",
    "        data = []\n",
    "        for row in reader:\n",
    "            cnt += 1;\n",
    "            if (cnt == 1):\n",
    "                continue;\n",
    "            if(file_name == \"test.csv\"): self.test_id.append(row[0])\n",
    "            #self.item\n",
    "            item = int(row[1]);\n",
    "            if (item not in self.item2id):\n",
    "                idx = len(self.item2id)\n",
    "                self.item2id[item] = idx;\n",
    "                self.id2item[idx] = item;\n",
    "                self.item_title[idx] = row[2].lower();    \n",
    "            sample = {'id': int(row[0]), 'x': self.item2id[item], 'y': row[3].lower()}\n",
    "            if (len(row) == 5):\n",
    "                rate = float(row[-1])\n",
    "                sample['r'] = rate;\n",
    "            data.append(sample);\n",
    "        return data;\n",
    "    \n",
    "    def load_description(self,):\n",
    "        reader = csv.reader(open('product_descriptions.csv', encoding='latin-1'));\n",
    "        cnt = 0;\n",
    "        for row in reader:\n",
    "            cnt += 1;\n",
    "            if (cnt == 1):\n",
    "                continue;\n",
    "            \n",
    "            item = int(row[0]);\n",
    "            idx = self.item2id[item]\n",
    "            self.item_text[idx] = row[1].lower()\n",
    "        return ;\n",
    "\n",
    "\n",
    "    def preprocess(self,):\n",
    "        transformer = text.TfidfVectorizer(max_features = 270000, stop_words ='english',tokenizer=LemmaTokenizer());\n",
    "        n = len(self.item2id);\n",
    "        corpus = []\n",
    "        item_title = []\n",
    "        item_text = []\n",
    "        for i in range(n):\n",
    "            corpus.append(self.item_text[i] + ' ' + self.item_title[i]);\n",
    "            item_title.append(self.item_title[i])\n",
    "            item_text.append(self.item_text[i])\n",
    "            #self.title.append(wordcount(self.item_title[i],self.item_text[i]))\n",
    "        self.title = np.asarray(self.title)\n",
    "        transformer.fit(corpus);\n",
    "        self.item_title_tfidf = transformer.transform(item_title);\n",
    "        self.item_text_tfidf = transformer.transform(item_text);\n",
    "        train_s = []\n",
    "        for item in self.train:\n",
    "            train_s.append(item['y']);\n",
    "        test_s = []\n",
    "        for item in self.test:\n",
    "            test_s.append(item['y']);\n",
    "        self.train_s = transformer.transform(train_s);\n",
    "        self.test_s = transformer.transform(test_s);\n",
    "        \n",
    "    def compute_cosine(self, inputs, data):\n",
    "        cosine = []\n",
    "        print(inputs[0].shape[0])\n",
    "        for i in range(inputs[0].shape[0]):\n",
    "            x = data[i]['x'];\n",
    "            a = inputs[0][i];b = inputs[1][x];c = inputs[2][x];\n",
    "            ab = cosine_similarity(a,b)[0][0];\n",
    "            bc = cosine_similarity(b,c)[0][0];\n",
    "            ac = cosine_similarity(a,c)[0][0];\n",
    "            cosine.append([ab,bc,ac]);\n",
    "        return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin proprocessing\n",
      "compute cosine\n",
      "74067\n",
      "166693\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "Data = Loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount(a,b):\n",
    "    count = 0\n",
    "    for ch in a:\n",
    "        if ch in b:\n",
    "            count += b.count(ch)\n",
    "    return count\n",
    "#self.item_title[item['x']]\n",
    "#print(Data.title.shape,Data.title[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 3, 1, 2, 0, 1, 2, 0, 4, 1, 1, 3, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 0, 0, 0, 4, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 0, 1, 0, 0, 0, 1, 0, 0, 2, 4, 2, 1, 1, 1, 2, 1, 1, 1, 2, 4, 4, 1, 1, 0, 2, 0, 1, 3, 2, 2, 2, 1, 1, 3, 0, 0, 2, 3, 2, 3, 2, 2, 2, 3, 0, 3, 1, 2, 1, 1, 0]\n",
      "(74067, 5)\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "def fuzzy(s1, s2):\n",
    "    return fuzz.token_set_ratio(s1, s2) / 100.0\n",
    "\n",
    "train_r = [];\n",
    "train_t = []\n",
    "len_train_y = []\n",
    "#title:query  number of common words\n",
    "common = []\n",
    "#description:query  number of common words\n",
    "common1 = []\n",
    "fuzz_title = []\n",
    "fuzz_des = []\n",
    "for i in range(len(Data.train)):\n",
    "    item = Data.train[i];\n",
    "    train_r.append(item['r']);\n",
    "    train_t.append(item['x'])\n",
    "    len_train_y.append(len(item['y'].split()))\n",
    "    common.append(wordcount(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "    common1.append(wordcount(Data.item_text[item['x']].split(),item['y'].split()))\n",
    "    fuzz_title.append(fuzzy(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "    fuzz_des.append(fuzzy(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "    \n",
    "    #wordcount(Data.item_title[item['id']],item['y'])\n",
    "#train_t = Data.item_tfidf[train_t]\n",
    "#train_s = sparse.hstack([train_t, Data.train_s])\n",
    "print(common[:100])\n",
    "\n",
    "\n",
    "#########\n",
    "r,c = Data.train_cosine.shape\n",
    "train_s = np.zeros((r,5))\n",
    "#train_s[:,:-5] = Data.train_cosine\n",
    "train_s[:,-5] = fuzz_des\n",
    "train_s[:,-4] = fuzz_title\n",
    "train_s[:,-3] = common1\n",
    "train_s[:,-2] = common\n",
    "#train_s = Data.train_cosine\n",
    "\n",
    "for i in range(len(train_s)):\n",
    "    train_s[i][-1] = len_train_y[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "#train_s = Data.train_cosine.reshape(-1,4)\n",
    "#b.reshape(-1,3)\n",
    "\n",
    "# print(b.shape)\n",
    "print(train_s.shape)\n",
    "# print(b[:3])\n",
    "# print(train_s[:3])\n",
    "train_r = np.asarray(train_r)\n",
    "#avg_r = np.mean(train_r);\n",
    "#train_r = train_r - avg_r;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 3, 1, 2, 0, 1, 2, 0, 4, 1, 1, 3, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 0, 0, 0, 4, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 0, 1, 0, 0, 0, 1, 0, 0, 2, 4, 2, 1, 1, 1, 2, 1, 1, 1, 2, 4, 4, 1, 1, 0, 2, 0, 1, 3, 2, 2, 2, 1, 1, 3, 0, 0, 2, 3, 2, 3, 2, 2, 2, 3, 0, 3, 1, 2, 1, 1, 0]\n",
      "(74067, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train_r = [];\n",
    "# train_t = []\n",
    "# len_train_y = []\n",
    "# #title:query  number of common words\n",
    "# common = []\n",
    "# #description:query  number of common words\n",
    "# common1 = []\n",
    "\n",
    "# for i in range(len(Data.train)):\n",
    "#     item = Data.train[i];\n",
    "#     train_r.append(item['r']);\n",
    "#     train_t.append(item['x'])\n",
    "#     len_train_y.append(len(item['y'].split()))\n",
    "#     common.append(wordcount(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "#     common1.append(wordcount(Data.item_text[item['x']].split(),item['y'].split()))\n",
    "    \n",
    "#     #wordcount(Data.item_title[item['id']],item['y'])\n",
    "# #train_t = Data.item_tfidf[train_t]\n",
    "# #train_s = sparse.hstack([train_t, Data.train_s])\n",
    "# print(common[:100])\n",
    "\n",
    "\n",
    "# #########\n",
    "# r,c = Data.train_cosine.shape\n",
    "# #train_s = np.zeros((r,c+5))\n",
    "# #train_s = np.zeros((r,c+5))\n",
    "# train_s = np.zeros((r,5))\n",
    "# train_s[:,:-5] = Data.train_cosine\n",
    "# train_s[:,-5] = fuzz_title_test\n",
    "# train_s[:,-4] = fuzz_des_test\n",
    "# train_s[:,-3] = common1\n",
    "# train_s[:,-2] = common\n",
    "# #train_s = Data.train_cosine\n",
    "\n",
    "# for i in range(len(train_s)):\n",
    "#     train_s[i][-1] = len_train_y[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "# #train_s = Data.train_cosine.reshape(-1,4)\n",
    "# #b.reshape(-1,3)\n",
    "\n",
    "# # print(b.shape)\n",
    "# print(train_s.shape)\n",
    "# # print(b[:3])\n",
    "# # print(train_s[:3])\n",
    "# train_r = np.asarray(train_r)\n",
    "# #avg_r = np.mean(train_r);\n",
    "# #train_r = train_r - avg_r;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74067, 8)\n",
      "{'alpha': 30.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001}\n",
      "[ 0.42631762 -0.26884102  0.46593481  0.16730788  0.16730788  0.00332264\n",
      " -0.00132144 -0.04340472]\n"
     ]
    }
   ],
   "source": [
    "#lr section\n",
    "\n",
    "model = linear_model.Ridge(alpha = 30.)\n",
    "print(train_s.shape)\n",
    "model.fit(train_s, train_r)\n",
    "print(model.get_params(deep=True))\n",
    "print(model.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1379982062806514\n"
     ]
    }
   ],
   "source": [
    "print(model.score(train_s,train_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "166693 3\n",
      "(166693, 8)\n",
      "(166693, 8)\n",
      "[2.07496909 1.99178892 2.33386842 ... 2.68052219 2.43481004 2.35174091]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_t = []\n",
    "len_test_y = []\n",
    "common_test = []\n",
    "common_test1 = []\n",
    "fuzz_title_test = []\n",
    "fuzz_des_test = []\n",
    "for i in range(len(Data.test)):\n",
    "    if(i%1000==0):print(i)\n",
    "    item = Data.test[i];\n",
    "    test_t.append(item['x'])\n",
    "    len_test_y.append(len(item['y'].split()))\n",
    "    common_test.append(wordcount(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "    common_test1.append(wordcount(Data.item_text[item['x']].split(),item['y'].split()))\n",
    "    fuzz_title_test.append(fuzzy(Data.item_text[item['x']].split(),item['y'].split()))\n",
    "    fuzz_des_test.append(fuzzy(Data.item_text[item['x']].split(),item['y'].split()))\n",
    "    \n",
    "#test_t = Data.item_tfidf[test_t]\n",
    "#test_t = Data.item_tfidf[test_t]\n",
    "#test_s = sparse.hstack([test_t, Data.test_s])\n",
    "\n",
    "#\n",
    "r,c = Data.test_cosine.shape\n",
    "print(r,c)\n",
    "#test_s = np.zeros((r,c+5))\n",
    "#test_s[:,:-5] = Data.test_cosine\n",
    "test_s = np.zeros((r,5))\n",
    "test_s[:,-5] = fuzz_des_test\n",
    "test_s[:,-4] = fuzz_title_test\n",
    "test_s[:,-3] = common_test1\n",
    "test_s[:,-2] = common_test\n",
    "\n",
    "print(test_s.shape)\n",
    "\n",
    "\n",
    "for i in range(len(len_test_y)):\n",
    "    test_s[i][-1] = len_test_y[i]\n",
    "\n",
    "\n",
    "#test_s = Data.test_cosine.reshape(-1,3)\n",
    "test_r = model.predict(test_s)\n",
    "\n",
    "print(test_s.shape)\n",
    "print(test_r)\n",
    "len(test_r)\n",
    "for i in range(len(test_r)):\n",
    "    if (test_r[i]<1): test_r[i] = 1\n",
    "    if (test_r[i]>3): test_r[i] = 3\n",
    "op = np.asarray(test_r)\n",
    "np.savetxt(\"submission.csv\", op, delimiter=\",\")\n",
    "f = open(\"submission.csv\", \"w\")\n",
    "f.write(\"{},{}\\n\".format(\"id\",\"relevance\"))\n",
    "\n",
    "# true_ids = []\n",
    "# for i in ids:\n",
    "#     true_ids.append(Data.id2item[i])\n",
    "for i in range(len(test_r)):\n",
    "    f.write(\"{},{}\\n\".format(Data.test_id[i], test_r[i]))\n",
    "\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"end of lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest setion\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model2 =RandomForestRegressor(max_depth=2, random_state=0)\n",
    "model2.fit(train_s, train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "166693 3\n",
      "(166693, 5)\n",
      "[2.17164586 2.10572163 2.29484725 ... 2.61870331 2.47921693 2.59006313]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"finished\")\n",
    "test_t = []\n",
    "len_test_y = []\n",
    "common_test = []\n",
    "for i in range(len(Data.test)):\n",
    "    item = Data.test[i];\n",
    "    test_t.append(item['x'])\n",
    "    len_test_y.append(len(item['y'].split()))\n",
    "    common_test.append(wordcount(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "#test_t = Data.item_tfidf[test_t]\n",
    "#test_s = sparse.hstack([test_t, Data.test_s])\n",
    "\n",
    "#\n",
    "r,c = Data.test_cosine.shape\n",
    "print(r,c)\n",
    "test_s = np.zeros((r,c+2))\n",
    "test_s[:,-2] = common_test\n",
    "test_s[:,:-2] = Data.test_cosine\n",
    "\n",
    "\n",
    "for i in range(len(len_test_y)):\n",
    "    test_s[i][-1] = len_test_y[i]\n",
    "\n",
    "\n",
    "#test_s = Data.test_cosine.reshape(-1,3)\n",
    "test_r = model2.predict(test_s)\n",
    "\n",
    "print(test_s.shape)\n",
    "print(test_r)\n",
    "len(test_r)\n",
    "for i in range(len(test_r)):\n",
    "    if (test_r[i]<1): test_r[i] = 1\n",
    "    if (test_r[i]>3): test_r[i] = 3\n",
    "op = np.asarray(test_r)\n",
    "np.savetxt(\"submission.csv\", op, delimiter=\",\")\n",
    "f = open(\"submission.csv\", \"w\")\n",
    "f.write(\"{},{}\\n\".format(\"id\",\"relevance\"))\n",
    "\n",
    "# true_ids = []\n",
    "# for i in ids:\n",
    "#     true_ids.append(Data.id2item[i])\n",
    "for i in range(len(test_r)):\n",
    "    f.write(\"{},{}\\n\".format(Data.test_id[i], test_r[i]))\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
