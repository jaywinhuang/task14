{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is the final version of our implementation. The features that we have implemented including TFIDF based cosine similary between product title and product description, product description and search terms and product title and search terms. Besides, we also include query length as a feature. Common word count is also calculated between product title and search terms as well as between product description and search terms. To futher improve the performance, Jaccard similarity between product title and search terms and between product description and search terms is also included as our feature. For model selection, we have implemented linear regression, random forest and SVR as our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part we basically preprocess the data, including removing stop words, conversion to lower case and use Lemmatizer.\n",
    "The reason why we leave out spell check is that somehow it makes the performance worse.\n",
    "Moreover, we specify a data structure here to store the complicated input data for easier use. Since information about\n",
    "one product-search pair is scattered in many files and we also want some id2item and item2id dictionary to help us get\n",
    "information in a handy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import sklearn.feature_extraction.text as text;\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from sklearn import linear_model\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import threading  \n",
    "from nltk import word_tokenize  \n",
    "from nltk import WordNetLemmatizer          \n",
    "#from nltk.stem import WordNet \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "class Loader:\n",
    "    #initialization and specify the data structure needed to store info\n",
    "    def __init__(self,):\n",
    "        #file path to train and test set\n",
    "        train_file = 'train.csv';\n",
    "        test_file = 'test.csv';\n",
    "        #make item2id and id2item dictionary\n",
    "        self.item2id = {}\n",
    "        self.id2item = {}\n",
    "        #item to title dictionary\n",
    "        self.item_title = {}\n",
    "        #Item to description dictionary\n",
    "        self.item_text = {}\n",
    "        #record id in test set for final output\n",
    "        self.test_id = []\n",
    "        #call load function\n",
    "        self.train = self.load(train_file);\n",
    "        self.test = self.load(test_file);\n",
    "        self.load_description();\n",
    "        #storing title string..\n",
    "        self.title = []\n",
    "        \n",
    "        print('begin proprocessing')\n",
    "        self.preprocess();\n",
    "        #compute cosine similarity based on tfidf of product title and product description,\n",
    "        #product description and search terms and product title and search terms for training set\n",
    "        print('compute cosine')\n",
    "        self.train_cosine = self.compute_cosine([self.train_s, self.item_title_tfidf, self.item_text_tfidf], self.train);\n",
    "        self.train_cosine = np.asarray(self.train_cosine);\n",
    "        #compute cosine similarity based on tfidf of product title and product description,\n",
    "        #product description and search terms and product title and search terms for test set\n",
    "        self.test_cosine = self.compute_cosine([self.test_s, self.item_title_tfidf, self.item_text_tfidf], self.test);\n",
    "        self.test_cosine = np.asarray(self.test_cosine)\n",
    "        print('finish')\n",
    "        \n",
    "    #Load the data\n",
    "    def load(self, file_name):\n",
    "        reader = csv.reader(open(file_name, encoding='latin-1'));\n",
    "        cnt = 0;\n",
    "        data = []\n",
    "        for row in reader:\n",
    "            cnt += 1;\n",
    "            if (cnt == 1):\n",
    "                continue;\n",
    "            if(file_name == \"test.csv\"): self.test_id.append(row[0])\n",
    "            #self.item\n",
    "            item = int(row[1]);\n",
    "            if (item not in self.item2id):\n",
    "                idx = len(self.item2id)\n",
    "                self.item2id[item] = idx;\n",
    "                self.id2item[idx] = item;\n",
    "                self.item_title[idx] = row[2].lower();    \n",
    "            sample = {'id': int(row[0]), 'x': self.item2id[item], 'y': row[3].lower()}\n",
    "            if (len(row) == 5):\n",
    "                rate = float(row[-1])\n",
    "                sample['r'] = rate;\n",
    "            data.append(sample);\n",
    "        return data;\n",
    "    #load product description\n",
    "    def load_description(self,):\n",
    "        reader = csv.reader(open('product_descriptions.csv', encoding='latin-1'));\n",
    "        cnt = 0;\n",
    "        for row in reader:\n",
    "            cnt += 1;\n",
    "            if (cnt == 1):\n",
    "                continue;\n",
    "            \n",
    "            item = int(row[0]);\n",
    "            idx = self.item2id[item]\n",
    "            self.item_text[idx] = row[1].lower()\n",
    "        return\n",
    "\n",
    "    #proprocess including removing stop words, conversion to lower case and use of Lemmatizer.\n",
    "    #and transform the data into right form\n",
    "    def preprocess(self,):\n",
    "        \n",
    "        transformer = text.TfidfVectorizer(max_features = 100000, stop_words ='english',tokenizer=LemmaTokenizer());\n",
    "        n = len(self.item2id);\n",
    "        corpus = []\n",
    "        item_title = []\n",
    "        item_text = []\n",
    "        for i in range(n):\n",
    "            corpus.append(self.item_text[i] + ' ' + self.item_title[i]);\n",
    "            item_title.append(self.item_title[i])\n",
    "            item_text.append(self.item_text[i])\n",
    "            #self.title.append(wordcount(self.item_title[i],self.item_text[i]))\n",
    "        self.title = np.asarray(self.title)\n",
    "        transformer.fit(corpus);\n",
    "        self.item_title_tfidf = transformer.transform(item_title);\n",
    "        self.item_text_tfidf = transformer.transform(item_text);\n",
    "        train_s = []\n",
    "        for item in self.train:\n",
    "            train_s.append(item['y']);\n",
    "        test_s = []\n",
    "        for item in self.test:\n",
    "            test_s.append(item['y']);\n",
    "        self.train_s = transformer.transform(train_s);\n",
    "        self.test_s = transformer.transform(test_s);\n",
    "        \n",
    "    def compute_cosine(self, inputs, data):\n",
    "        cosine = []\n",
    "        print(inputs[0].shape[0])\n",
    "        for i in range(inputs[0].shape[0]):\n",
    "            x = data[i]['x'];\n",
    "            a = inputs[0][i];b = inputs[1][x];c = inputs[2][x];\n",
    "            ab = cosine_similarity(a,b)[0][0];\n",
    "            bc = cosine_similarity(b,c)[0][0];\n",
    "            ac = cosine_similarity(a,c)[0][0];\n",
    "            cosine.append([ab,bc,ac]);\n",
    "        return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading\n",
      "1527214902.0816398\n",
      "begin proprocessing\n",
      "compute cosine\n",
      "74067\n",
      "166693\n",
      "finish\n",
      "1527215905.435162\n"
     ]
    }
   ],
   "source": [
    "print(\"start loading\")\n",
    "import time\n",
    "print(time.time())\n",
    "Data = Loader()\n",
    "print(time.time())\n",
    "#913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of common word give two list of strings.\n",
    "def wordcount(a,b):\n",
    "    count = 0\n",
    "    for ch in a:\n",
    "        if ch in b:\n",
    "            count += b.count(ch)\n",
    "    return count\n",
    "#self.item_title[item['x']]\n",
    "#print(Data.title.shape,Data.title[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 3, 1, 2, 0, 1, 2, 0, 4, 1, 1, 3, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 0, 0, 0, 4, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 0, 1, 0, 0, 0, 1, 0, 0, 2, 4, 2, 1, 1, 1, 2, 1, 1, 1, 2, 4, 4, 1, 1, 0, 2, 0, 1, 3, 2, 2, 2, 1, 1, 3, 0, 0, 2, 3, 2, 3, 2, 2, 2, 3, 0, 3, 1, 2, 1, 1, 0]\n",
      "(74067, 6)\n"
     ]
    }
   ],
   "source": [
    "#Calculate fuzz similarity between product title and search terms as well as between product description and \n",
    "#search terms\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "def fuzzy(s1, s2):\n",
    "    return fuzz.token_set_ratio(s1, s2) / 100.0\n",
    "\n",
    "train_r = []\n",
    "train_t = []\n",
    "len_train_y = []\n",
    "#title:query  number of common words\n",
    "common = []\n",
    "#description:query  number of common words\n",
    "common1 = []\n",
    "##fuzz_title = []\n",
    "##fuzz_des = []\n",
    "for i in range(len(Data.train)):\n",
    "    item = Data.train[i];\n",
    "    train_r.append(item['r']);\n",
    "    train_t.append(item['x'])\n",
    "    len_train_y.append(len(item['y'].split()))\n",
    "    common.append(wordcount(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "    common1.append(wordcount(Data.item_text[item['x']].split(),item['y'].split()))\n",
    "    ##fuzz_title.append(fuzzy(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "    ##fuzz_des.append(fuzzy(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "    \n",
    "    #wordcount(Data.item_title[item['id']],item['y'])\n",
    "#train_t = Data.item_tfidf[train_t]\n",
    "#train_s = sparse.hstack([train_t, Data.train_s])\n",
    "print(common[:100])\n",
    "\n",
    "\n",
    "#########\n",
    "r,c = Data.train_cosine.shape\n",
    "train_s = np.zeros((r,c+3))\n",
    "train_s[:,:-3] = Data.train_cosine\n",
    "# train_s[:,-5] = fuzz_des\n",
    "# train_s[:,-4] = fuzz_title\n",
    "train_s[:,-3] = common1\n",
    "train_s[:,-2] = common\n",
    "#train_s = Data.train_cosine\n",
    "\n",
    "for i in range(len(train_s)):\n",
    "    train_s[i][-1] = len_train_y[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "#train_s = Data.train_cosine.reshape(-1,4)\n",
    "#b.reshape(-1,3)\n",
    "\n",
    "# print(b.shape)\n",
    "print(train_s.shape)\n",
    "# print(b[:3])\n",
    "# print(train_s[:3])\n",
    "train_r = np.asarray(train_r)\n",
    "#avg_r = np.mean(train_r);\n",
    "#train_r = train_r - avg_r;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_r = [];\n",
    "# train_t = []\n",
    "# len_train_y = []\n",
    "# #title:query  number of common words\n",
    "# common = []\n",
    "# #description:query  number of common words\n",
    "# common1 = []\n",
    "\n",
    "# for i in range(len(Data.train)):\n",
    "#     item = Data.train[i];\n",
    "#     train_r.append(item['r']);\n",
    "#     train_t.append(item['x'])\n",
    "#     len_train_y.append(len(item['y'].split()))\n",
    "#     common.append(wordcount(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "#     common1.append(wordcount(Data.item_text[item['x']].split(),item['y'].split()))\n",
    "    \n",
    "#     #wordcount(Data.item_title[item['id']],item['y'])\n",
    "# #train_t = Data.item_tfidf[train_t]\n",
    "# #train_s = sparse.hstack([train_t, Data.train_s])\n",
    "# print(common[:100])\n",
    "\n",
    "\n",
    "# #########\n",
    "# r,c = Data.train_cosine.shape\n",
    "# #train_s = np.zeros((r,c+5))\n",
    "# #train_s = np.zeros((r,c+5))\n",
    "# train_s = np.zeros((r,5))\n",
    "# train_s[:,:-5] = Data.train_cosine\n",
    "# train_s[:,-5] = fuzz_title_test\n",
    "# train_s[:,-4] = fuzz_des_test\n",
    "# train_s[:,-3] = common1\n",
    "# train_s[:,-2] = common\n",
    "# #train_s = Data.train_cosine\n",
    "\n",
    "# for i in range(len(train_s)):\n",
    "#     train_s[i][-1] = len_train_y[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "# #train_s = Data.train_cosine.reshape(-1,4)\n",
    "# #b.reshape(-1,3)\n",
    "\n",
    "# # print(b.shape)\n",
    "# print(train_s.shape)\n",
    "# # print(b[:3])\n",
    "# # print(train_s[:3])\n",
    "# train_r = np.asarray(train_r)\n",
    "# #avg_r = np.mean(train_r);\n",
    "# #train_r = train_r - avg_r;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74067, 6)\n",
      "{'alpha': 50.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001}\n",
      "[ 0.54507024 -0.27813751  0.44060243  0.00396836  0.04111296 -0.06298762]\n"
     ]
    }
   ],
   "source": [
    "#lr section\n",
    "\n",
    "model = linear_model.Ridge(alpha = 50.)\n",
    "print(train_s.shape)\n",
    "model.fit(train_s, train_r)\n",
    "print(model.get_params(deep=True))\n",
    "print(model.coef_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest setion\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model2 =RandomForestRegressor(max_depth=2, random_state=0)\n",
    "model2.fit(train_s, train_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5220361  -0.24111234  0.36205871 -0.00298779  0.05372537 -0.09750834]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "model4 = LinearSVR(random_state=0)\n",
    "model4.fit(train_s, train_r) \n",
    "print(model4.coef_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svr\n",
    "from sklearn.svm import SVR\n",
    "model3 = SVR(C=1.0, epsilon=0.2)\n",
    "model3.fit(train_s, train_r) \n",
    "#print(model3.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.score(train_s,train_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Prediction and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166693 3\n",
      "(166693, 6)\n",
      "start predicting\n",
      "finished\n",
      "(166693, 6)\n",
      "[2.12642291 2.1007307  2.32706934 ... 2.60175378 2.50972434 2.4940787 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_t = []\n",
    "len_test_y = []\n",
    "common_test = []\n",
    "common_test1 = []\n",
    "#fuzz_title_test = []\n",
    "#fuzz_des_test = []\n",
    "for i in range(len(Data.test)):\n",
    "    #if(i%1000==0):print(i)\n",
    "    item = Data.test[i];\n",
    "    test_t.append(item['x'])\n",
    "    len_test_y.append(len(item['y'].split()))\n",
    "    common_test.append(wordcount(Data.item_title[item['x']].split(),item['y'].split()))\n",
    "    common_test1.append(wordcount(Data.item_text[item['x']].split(),item['y'].split()))\n",
    "    #fuzz_title_test.append(fuzzy(Data.item_text[item['x']].split(),item['y'].split()))\n",
    "    #fuzz_des_test.append(fuzzy(Data.item_text[item['x']].split(),item['y'].split()))\n",
    "    \n",
    "#test_t = Data.item_tfidf[test_t]\n",
    "#test_t = Data.item_tfidf[test_t]\n",
    "#test_s = sparse.hstack([test_t, Data.test_s])\n",
    "\n",
    "#\n",
    "r,c = Data.test_cosine.shape\n",
    "print(r,c)\n",
    "#test_s = np.zeros((r,c+5))\n",
    "test_s = np.zeros((r,c+3))\n",
    "test_s[:,:-3] = Data.test_cosine\n",
    "#test_s[:,-5] = fuzz_des_test\n",
    "#test_s[:,-4] = fuzz_title_test\n",
    "test_s[:,-3] = common_test1\n",
    "test_s[:,-2] = common_test\n",
    "\n",
    "print(test_s.shape)\n",
    "\n",
    "\n",
    "for i in range(len(len_test_y)):\n",
    "    test_s[i][-1] = len_test_y[i]\n",
    "\n",
    "\n",
    "#test_s = Data.test_cosine.reshape(-1,3)\n",
    "print(\"start predicting\")\n",
    "test_r = model.predict(test_s)\n",
    "print(\"finished\")\n",
    "\n",
    "print(test_s.shape)\n",
    "print(test_r)\n",
    "len(test_r)\n",
    "for i in range(len(test_r)):\n",
    "    if (test_r[i]<1): test_r[i] = 1\n",
    "    if (test_r[i]>3): test_r[i] = 3\n",
    "        \n",
    "#make output\n",
    "op = np.asarray(test_r)\n",
    "np.savetxt(\"submission.csv\", op, delimiter=\",\")\n",
    "f = open(\"submission.csv\", \"w\")\n",
    "f.write(\"{},{}\\n\".format(\"id\",\"relevance\"))\n",
    "\n",
    "# true_ids = []\n",
    "# for i in ids:\n",
    "#     true_ids.append(Data.id2item[i])\n",
    "for i in range(len(test_r)):\n",
    "    f.write(\"{},{}\\n\".format(Data.test_id[i], test_r[i]))\n",
    "\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3",
   "language": "python",
   "name": "py3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
