{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity features (Version 1)\n",
    "A simple baseline that generates features based on the cosine similarities of the search query with the product description, product title and attribute name vectors. This model first builds the vocab of words under a column and then finds the representations of the words in each training example. After that, it computes the tfidf vectors of the columns in question. With these tfidf values in hand, the tfidf values of the search query are computed; the resultant being 2 vectors - document and search vectors. Each feature is the cosine similarity of a document vector with the corresponding search vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method takes a couple of vectors - the document and search vectors and returns the cosine similarity between them. The first step is to fill in the values of the search vector. The vectors are both sparse representations of the tfidf values of the words in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the document and search vectors and returns the cosine similarity of the 2 vectors.\n",
    "def computeCosineSim(words, doc_vector, search_vector, vectorizer):\n",
    "    # First store the term frequencies of the words\n",
    "    dict = {}\n",
    "    for word in words:\n",
    "        if word in dict:\n",
    "            count = dict.get(word)\n",
    "            count = count + 1\n",
    "            dict[word] = count\n",
    "        else:\n",
    "            dict[word] = 1\n",
    "\n",
    "    doc_array = doc_vector.toarray()\n",
    "    search_array = search_vector.toarray()\n",
    "    # Now compute the tfidf of the words wrt to the document to get the search vector.\n",
    "    for word in dict:\n",
    "        index = vectorizer.vocabulary_.get(word, -1)\n",
    "        # print(word + \" \" + str(index))\n",
    "        if index != -1:\n",
    "            search_array[0][index] = (doc_array[0][index] * dict[word]) / len(words)  # Has to be reconsidered.\n",
    "    return cosine_similarity(doc_array, search_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in extracting the features is merging the files. There are 3 files to work on in the preprocessing step - attributes.csv, train.csv, product_descriptions.csv. Motivation - We need a combined vocab of descriptions, titles and attributes in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method takes the different product files and combines them into one on product_id.\n",
    "def preprocess():\n",
    "    # Combining the rows of the attributes file on product_uid.\n",
    "    myfile_att = open(\"att_mod.csv\", \"w\")\n",
    "    df_attributes = pd.read_csv(\"attributes.csv\", encoding='latin-1')\n",
    "    df_attributes['name'] = df_attributes['name'].astype(str) + \" \"\n",
    "    df_attributes['value'] = df_attributes['value'].astype(str) + \" \"\n",
    "    df_attributes = df_attributes.groupby('product_uid').apply(lambda x: x.sum())\n",
    "    df_attributes = df_attributes.drop(df_attributes.columns[[0]], axis=1)\n",
    "    df_attributes.to_csv(myfile_att, sep=',', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    myfile_att.close()\n",
    "\n",
    "    df_attributes = pd.read_csv(\"att_mod.csv\", encoding='latin-1')\n",
    "    df_prodDesc = pd.read_csv(\"product_descriptions.csv\", encoding='latin-1')\n",
    "    result = pd.merge(pd.DataFrame(df_prodDesc), pd.DataFrame(df_attributes), on='product_uid', how='left')\n",
    "    merged_file = open(\"merged.csv\", \"w\")\n",
    "    # result.rename(columns={result.columns[-1]: \"id\"}, inplace=True)\n",
    "    result.to_csv(merged_file, sep=',', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    merged_file.close()\n",
    "\n",
    "    # Merge the prod_titles in the train file with the product details.\n",
    "    df_train = pd.read_csv(\"train.csv\", encoding='latin-1')\n",
    "    df_merged = pd.read_csv(\"merged.csv\", encoding='latin-1')\n",
    "    result = pd.merge(pd.DataFrame(df_merged),\n",
    "        pd.DataFrame(df_train)[['product_uid', 'product_title','search_term','relevance']], on='product_uid', how='inner')\n",
    "    merged_file = open(\"training.csv\", \"w\")\n",
    "    result.to_csv(merged_file, sep=',', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    merged_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the corpus:\n",
    "This method takes care of vectorizing the text under a certain column in a csv file. The tfidfVectorizer() builds and fits the vocab. Each training example can then be transformed before finding the cosine similarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the column to vectorize, this function builds the vocab from words under\n",
    "# the column, gets the tfidf vector representation, computes and returns the cosine similarity matrix.\n",
    "def vectorize(column):\n",
    "    # Get the product titles.\n",
    "    df_merged.fillna(' ', inplace=True)\n",
    "    text = df_merged[column].values.astype('U')\n",
    "\n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(text)\n",
    "\n",
    "    cosine = []\n",
    "    # Encode each document based on the transform.\n",
    "    for t in range(0, len(df_merged)):\n",
    "        doc_vector = vectorizer.transform([df_merged[column][t]])\n",
    "        # print(doc_vector.toarray())\n",
    "\n",
    "        # Now get the TFIDF of the search query.\n",
    "        search_vector = vectorizer.transform([df_merged['search_term'][t]])\n",
    "        # print(search_vector.toarray())\n",
    "        # print(\"---------------\")\n",
    "        words = str(df_merged['search_term'][t]).split(\" \")\n",
    "        cos = computeCosineSim(words, doc_vector, search_vector, vectorizer)\n",
    "        # print(cos)\n",
    "        cosine.append(cos)\n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driver of the program.\n",
    "Flow of execution:\n",
    "\n",
    "1) Preprocess the data\n",
    "2) Get the cosine similarity values of the search vector with each of product description, product title and attributes vectors\n",
    "3) Put the vectors in a matrix and prints it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess()\n",
    "df_merged = pd.read_csv(\"training.csv\", encoding='latin-1')\n",
    "features = []\n",
    "cosine = vectorize('product_description')\n",
    "\n",
    "for c in cosine:\n",
    "    temp = []\n",
    "    temp.append(c[0][0])\n",
    "    features.append(temp)\n",
    "\n",
    "cosine = vectorize('product_title')\n",
    "for i in range(0, len(cosine)):\n",
    "    features[i].append(cosine[i][0][0])\n",
    "\n",
    "cosine = vectorize('name')\n",
    "for i in range(0, len(cosine)):\n",
    "    features[i].append(cosine[i][0][0])\n",
    "\n",
    "# This matrix can be fed into a Linear Regression model.\n",
    "print(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
